import numpy as np
import torch
from torch import nn
import os
from datetime import datetime
import argparse
import sys


def extract(tensor):
    """
    Converts tensor weight into readable string
    """
    np.set_printoptions(threshold=sys.maxsize)
    return np.array2string(tensor.detach().cpu().numpy().reshape((1,-1)).squeeze(), separator=',', floatmode='maxprec').strip("[]").replace('\n', '')

#Parse input arguments
parser = argparse.ArgumentParser()
parser.add_argument('Path', help="Path to model to convert to .h file", type=str)
args = parser.parse_args()
path = args.Path

#load model from path
model = torch.load(path, map_location=torch.device('cpu'))

if hasattr(model, "lstm"):
    model_type = "lstm"
elif hasattr(model, "gru"):
    model_type = "gru"
elif hasattr(model, "rnn"):
    model_type = "rnn"
else:
   raise ValueError("Invalid model type")
print("Model type: ", model_type)

#define parameters
if model_type == "lstm":
    hidden_size = model.lstm.hidden_size
    num_layers = model.lstm.num_layers
elif model_type == "gru":
    hidden_size = model.gru.hidden_size
    num_layers = model.gru.num_layers
elif model_type == "rnn":
    hidden_size = model.rnn.hidden_size
    num_layers = model.rnn.num_layers
else:
    raise ValueError("Invalid model type")
in_size = 1
print("Hidden size: {}, Num Layers: {}".format(hidden_size, num_layers))

#output file
write_path = 'headers/' + model_type + '-{}-{}.h'.format(hidden_size, num_layers)


#Clear file
header = open(write_path, 'w')
header.close()

#Write to file
header = open(write_path, 'a')

#Write header header (comments)
header.write("/**\n * Autogenerated header file for {}-{}-{}\n * neural network model parameters.\n * \n * Created on: {}\n * By Michael Holmes\n*/\n\n".format(model_type, hidden_size, num_layers, datetime.now().strftime("%d/%m/%Y at: %H:%M")))

#Start of file
header.write("#pragma once\n#include \"layers.h\"\n\ntemplate<typename T>\nclass {}_{}_{}\n{{\npublic:\n".format(model_type.capitalize(), hidden_size, num_layers))

#define constants
header.write("\tconst static int h = {};\n".format(hidden_size))
header.write("\tconst static int d = {};\n".format(in_size))
header.write("\tconst static int layers = {};\n".format(num_layers))

#define layers
header.write("\n")
header.write("\tFccLayer<T, h, d> f;\n")
header.write("\t{}Layer<T, h, d> l0;\n".format(model_type.capitalize()))
for i in range(1, num_layers):
    header.write("\t{}Layer<T, h, h> l{};\n".format(model_type.capitalize(), i))


#Initialise model parameters
header.write("\n")
header.write("\t{}_{}_{}() {{\n".format(model_type.capitalize(), hidden_size, num_layers))

for name, param in model.named_parameters():
    first = name.split('.')
    second = first[1].split('_')

    #read param name
    layer_type = first[0]
    param_type = second[0]

    #Process LSTM
    if layer_type == 'lstm':
        #Get remaining params
        param_subscript = second[1]
        layer_num = int(second[2][1:])

        #input weight
        if param_type == 'weight' and param_subscript == 'ih':
            header.write("\t\tl{}.Wii << {};\n".format(layer_num, extract(param[:hidden_size,:])))
            header.write("\t\tl{}.Wif << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size,:])))
            header.write("\t\tl{}.Wig << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size,:])))
            header.write("\t\tl{}.Wio << {};\n".format(layer_num, extract(param[3*hidden_size:4*hidden_size,:])))
            header.write("\n")
        #hidden weight
        elif param_type == 'weight' and param_subscript == 'hh':
            header.write("\t\tl{}.Whi << {};\n".format(layer_num, extract(param[:hidden_size,:])))
            header.write("\t\tl{}.Whf << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size,:])))
            header.write("\t\tl{}.Whg << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size,:])))
            header.write("\t\tl{}.Who << {};\n".format(layer_num, extract(param[3*hidden_size:4*hidden_size,:])))
            header.write("\n")
        #input bias
        if param_type == 'bias' and param_subscript == 'ih':
            header.write("\t\tl{}.bii << {};\n".format(layer_num, extract(param[:hidden_size])))
            header.write("\t\tl{}.bif << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size])))
            header.write("\t\tl{}.big << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size])))
            header.write("\t\tl{}.bio << {};\n".format(layer_num, extract(param[3*hidden_size:4*hidden_size])))
            header.write("\n")
        #hidden bias
        elif param_type == 'bias' and param_subscript == 'hh':
            header.write("\t\tl{}.bhi << {};\n".format(layer_num, extract(param[:hidden_size])))
            header.write("\t\tl{}.bhf << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size])))
            header.write("\t\tl{}.bhg << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size])))
            header.write("\t\tl{}.bho << {};\n".format(layer_num, extract(param[3*hidden_size:4*hidden_size])))
            header.write("\n")
            #initiliase time reccurent
            header.write("\t\tl{}.htn1 << {};\n".format(layer_num, np.array2string(np.zeros(hidden_size).reshape((1,-1)).squeeze(), separator=',', floatmode='maxprec').strip("[]").replace('\n', '')))
            header.write("\t\tl{}.ctn1 << {};\n".format(layer_num, np.array2string(np.zeros(hidden_size).reshape((1,-1)).squeeze(), separator=',', floatmode='maxprec').strip("[]").replace('\n', '')))
            header.write("\n")  
        
    #Process GRU
    elif layer_type == 'gru':
        #Get remaining params
        param_subscript = second[1]
        layer_num = int(second[2][1:])

        #input weight
        if param_type == 'weight' and param_subscript == 'ih':
            header.write("\t\tl{}.Wir << {};\n".format(layer_num, extract(param[:hidden_size,:])))
            header.write("\t\tl{}.Wiz << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size,:])))
            header.write("\t\tl{}.Win << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size,:])))
            header.write("\n")
        #hidden weight
        elif param_type == 'weight' and param_subscript == 'hh':
            header.write("\t\tl{}.Whr << {};\n".format(layer_num, extract(param[:hidden_size,:])))
            header.write("\t\tl{}.Whz << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size,:])))
            header.write("\t\tl{}.Whn << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size,:])))
            header.write("\n")
        #input bias
        if param_type == 'bias' and param_subscript == 'ih':
            header.write("\t\tl{}.bir << {};\n".format(layer_num, extract(param[:hidden_size])))
            header.write("\t\tl{}.biz << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size])))
            header.write("\t\tl{}.bin << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size])))
            header.write("\n")
        #hidden bias
        elif param_type == 'bias' and param_subscript == 'hh':
            header.write("\t\tl{}.bhr << {};\n".format(layer_num, extract(param[:hidden_size])))
            header.write("\t\tl{}.bhz << {};\n".format(layer_num, extract(param[hidden_size:2*hidden_size])))
            header.write("\t\tl{}.bhn << {};\n".format(layer_num, extract(param[2*hidden_size:3*hidden_size])))
            header.write("\n")
            #initiliase time reccurent
            header.write("\t\tl{}.htn1 << {};\n".format(layer_num, np.array2string(np.zeros(hidden_size).reshape((1,-1)).squeeze(), separator=',', floatmode='maxprec').strip("[]").replace('\n', '')))
            header.write("\n")
        
    #Process GRU
    elif layer_type == 'rnn':
        #Get remaining params
        param_subscript = second[1]
        layer_num = int(second[2][1:])

        #input weight
        if param_type == 'weight' and param_subscript == 'ih':
            header.write("\t\tl{}.Wih << {};\n".format(layer_num, extract(param[:hidden_size,:])))
            header.write("\n")
        #hidden weight
        elif param_type == 'weight' and param_subscript == 'hh':
            header.write("\t\tl{}.Whh << {};\n".format(layer_num, extract(param[:hidden_size,:])))
            header.write("\n")
        #input bias
        if param_type == 'bias' and param_subscript == 'ih':
            header.write("\t\tl{}.bih << {};\n".format(layer_num, extract(param[:hidden_size])))
            header.write("\n")
        #hidden bias
        elif param_type == 'bias' and param_subscript == 'hh':
            header.write("\t\tl{}.bhh << {};\n".format(layer_num, extract(param[:hidden_size])))
            header.write("\n")
            #initiliase time reccurent
            header.write("\t\tl{}.htn1 << {};\n".format(layer_num, np.array2string(np.zeros(hidden_size).reshape((1,-1)).squeeze(), separator=',', floatmode='maxprec').strip("[]").replace('\n', '')))
            header.write("\n")

    #Process Linear
    elif layer_type == 'linear':
        if param_type == 'weight':
            header.write("\t\tf.A << {};\n".format(extract(param)))
        elif param_type == 'bias':
            header.write("\t\tf.b << {};\n".format(extract(param)))

#Finish constructor
header.write("\t}\n\n")

#Create apply model function

if num_layers == 1:
    header.write("\tvoid apply_model(T* x, T* y) {\n\t\tl0.apply_layer(*x);\n\t\t*y=f.apply_layer(l0.ht);\n\t}\n")
elif num_layers == 2:
    header.write("\tvoid apply_model(T* x, T* y) {\n\t\tl0.apply_layer(*x);\n\t\tl1.apply_layer(l0.ht);\n\t\t*y=f.apply_layer(l1.ht);\n\t}\n")
elif num_layers == 4:
    header.write("\tvoid apply_model(T* x, T* y) {\n\t\tl0.apply_layer(*x);\n\t\tl1.apply_layer(l0.ht);\n\t\tl2.apply_layer(l1.ht);\n\t\tl3.apply_layer(l2.ht);\n\t\t*y=f.apply_layer(l3.ht);\n\t}\n")

header.write("};")
header.close()